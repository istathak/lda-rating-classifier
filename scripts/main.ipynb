{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGENERATE_DATA = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import dataloader as dl\n",
    "import pickle\n",
    "import gzip\n",
    "from pathlib import Path\n",
    "\n",
    "pkl_path = \"amazon_reviews_preproc.pkl.gz\"\n",
    "if REGENERATE_DATA:\n",
    "    X_train, y_train, X_test, y_test = dl.load_data(\"/Users/ifigeneiastathaki/Desktop/projects/LDA&regression/data/raw_data/electronics_small.csv\")\n",
    "    with gzip.open(pkl_path, \"wb\") as f:\n",
    "        pickle.dump(\n",
    "            {\n",
    "                \"X_train\": X_train,\n",
    "                \"y_train\": y_train,\n",
    "                \"X_test\":  X_test,\n",
    "                \"y_test\":  y_test,\n",
    "            },\n",
    "            f,\n",
    "            protocol=pickle.HIGHEST_PROTOCOL,\n",
    "        )\n",
    "\n",
    "    print(f\"Saved preprocessed data to {pkl_path.resolve()}\")\n",
    "\n",
    "else:\n",
    "    with gzip.open(pkl_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    X_train = data[\"X_train\"]\n",
    "    y_train = data[\"y_train\"]\n",
    "    X_test  = data[\"X_test\"]\n",
    "    y_test  = data[\"y_test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BoW: 100%|██████████| 2399630/2399630 [04:22<00:00, 9155.63it/s]  \n"
     ]
    }
   ],
   "source": [
    "bow_corpus_train, id2word = dl.bagofwords(X_train)\n",
    "bow_corpus_test = [id2word.doc2bow(text) for text in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After validation: 227,939 docs\n",
      "Sample size: 227,939 docs (10.0% of full corpus)\n",
      "Training LDA: K=40, passes=3, iterations=25, workers=7\n",
      "LDA training complete\n"
     ]
    }
   ],
   "source": [
    "from utils.model import train_lda, corpus_to_dense\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import numpy as np\n",
    "\n",
    "sample_frac = 0.1 \n",
    "random_state  = 42\n",
    "\n",
    "sss = StratifiedShuffleSplit(n_splits=1,\n",
    "                             train_size=sample_frac,\n",
    "                             random_state=random_state)\n",
    "\n",
    "sample_idx, _ = next(sss.split(np.zeros(len(y_train)), y_train))\n",
    "\n",
    "bow_train_sample = [bow_corpus_train[i] for i in sample_idx]\n",
    "\n",
    "bow_train_sample = [\n",
    "    doc for doc in bow_train_sample\n",
    "    if isinstance(doc, list) and doc and isinstance(doc[0], tuple)\n",
    "]\n",
    "print(f\"After validation: {len(bow_train_sample):,d} docs\")\n",
    "\n",
    "print(f\"Sample size: {len(bow_train_sample):,d} docs \"\n",
    "      f\"({100*sample_frac:.1f}% of full corpus)\")\n",
    "\n",
    "lda = train_lda(bow_train_sample, id2word, k=40, passes=3, iterations=25, workers= None, return_dense=False)\n",
    "\n",
    "X_train_vec = corpus_to_dense(lda, bow_corpus_train) \n",
    "X_test_vec = corpus_to_dense(lda, bow_corpus_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ifigeneiastathaki/Desktop/projects/LDA&regression/env/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.361498763143682\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.150     0.515     0.232     41155\n",
      "           2      0.064     0.209     0.097     27232\n",
      "           3      0.117     0.157     0.134     45648\n",
      "           4      0.232     0.215     0.223    104056\n",
      "           5      0.758     0.420     0.541    381817\n",
      "\n",
      "    accuracy                          0.361    599908\n",
      "   macro avg      0.264     0.303     0.246    599908\n",
      "weighted avg      0.545     0.361     0.413    599908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from collections import defaultdict\n",
    "\n",
    "X_train = np.asarray(X_train_vec)  \n",
    "X_test  = np.asarray(X_test_vec)\n",
    "y_train = np.asarray(y_train)\n",
    "\n",
    "#\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "# find indices per class\n",
    "indices_by_class = defaultdict(list)\n",
    "for idx, label in enumerate(y_train):\n",
    "    indices_by_class[label].append(idx)\n",
    "\n",
    "min_size = min(len(lst) for lst in indices_by_class.values())\n",
    "\n",
    "balanced_idx = []\n",
    "for label, idx_list in indices_by_class.items():\n",
    "    balanced_idx.extend(rng.choice(idx_list, size=min_size, replace=False))\n",
    "\n",
    "balanced_idx = np.array(balanced_idx)\n",
    "X_balanced   = X_train[balanced_idx]\n",
    "y_balanced   = y_train[balanced_idx]\n",
    "\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l2\",          \n",
    "    C=1.0,                 \n",
    "    solver=\"lbfgs\",        \n",
    "    max_iter=1000,\n",
    "    n_jobs=-1,        \n",
    "    multi_class=\"multinomial\",  \n",
    "    class_weight = 'balanced'\n",
    ")\n",
    "\n",
    "# logreg= LogisticRegression(solver='saga', max_iter=2000,multi_class='multinomial')\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = logreg.predict(X_test)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LDA grid:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "train_lda() got an unexpected keyword argument 'return_dense'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     51\u001b[39m scores = []\n\u001b[32m     52\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m K \u001b[38;5;129;01min\u001b[39;00m tqdm(K_grid, desc=\u001b[33m\"\u001b[39m\u001b[33mLDA grid\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m     lda = \u001b[43mtrain_lda\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbow_train_sub\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mid2word\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[43m        \u001b[49m\u001b[43mk\u001b[49m\u001b[43m=\u001b[49m\u001b[43mK\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpasses\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m        \u001b[49m\u001b[43mworkers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dense\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# infer θ\u001b[39;00m\n\u001b[32m     60\u001b[39m     X_train_theta = corpus_to_dense(lda, bow_train_sub)\n",
      "\u001b[31mTypeError\u001b[39m: train_lda() got an unexpected keyword argument 'return_dense'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np, matplotlib.pyplot as plt, pickle, gzip\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from utils.model import train_lda, corpus_to_dense, train_logistic_regression\n",
    "import importlib, utils.model as ml\n",
    "ml = importlib.reload(ml)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# PARAMETERS\n",
    "K_grid        = [1,3]\n",
    "sample_frac   = 0.10         # first sweep\n",
    "val_frac      = 0.10         # inside that 10 %\n",
    "random_state  = 42\n",
    "out_dir       = Path(\"lda_grid\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1.  STRATIFIED sample 10 % for the sweep\n",
    "sss = StratifiedShuffleSplit(n_splits=1, train_size=sample_frac,\n",
    "                             random_state=random_state)\n",
    "sample_idx, _ = next(sss.split(np.zeros(len(y_train)), y_train))\n",
    "\n",
    "bow_sample = [bow_corpus_train[i] for i in sample_idx]\n",
    "tok_sample = [X_train[i]         for i in sample_idx]\n",
    "y_sample   = y_train[sample_idx]\n",
    "\n",
    "# remove empties\n",
    "mask       = [bool(doc) for doc in bow_sample]\n",
    "bow_sample = [d for d, ok in zip(bow_sample, mask) if ok]\n",
    "tok_sample = [t for t, ok in zip(tok_sample, mask) if ok]\n",
    "y_sample   = y_sample[mask]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2.  train/validation split INSIDE sample\n",
    "train_idx, val_idx = train_test_split(\n",
    "    np.arange(len(bow_sample)),\n",
    "    test_size=val_frac,\n",
    "    stratify=y_sample,\n",
    "    random_state=random_state,\n",
    ")\n",
    "bow_train_sub  = [bow_sample[i] for i in train_idx]\n",
    "bow_val_sub    = [bow_sample[i] for i in val_idx]\n",
    "y_train_sub    = y_sample[train_idx]\n",
    "y_val_sub      = y_sample[val_idx]\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 3.  grid search over K\n",
    "scores = []\n",
    "for K in tqdm(K_grid, desc=\"LDA grid\"):\n",
    "    lda = train_lda(\n",
    "        bow_train_sub, id2word,\n",
    "        k=K, passes=3, iterations=25,\n",
    "        workers=None\n",
    "    )\n",
    "\n",
    "    # infer θ\n",
    "    X_train_theta = corpus_to_dense(lda, bow_train_sub)\n",
    "    X_val_theta   = corpus_to_dense(lda, bow_val_sub)\n",
    "\n",
    "    # fixed logistic regressor (class-weight balanced)\n",
    "    clf = train_logistic_regression(\n",
    "        X_train_theta, y_train_sub,\n",
    "        balance=False, \n",
    "        class_weight = True,         # no undersample, we rely on class_weight\n",
    "        C=1.0,                  # keep logistic params constant\n",
    "        max_iter=2000\n",
    "    )\n",
    "\n",
    "    y_pred_val = clf.predict(X_val_theta)\n",
    "    f1 = f1_score(y_val_sub, y_pred_val, average=\"macro\")\n",
    "    scores.append((K, f1))\n",
    "    print(f\"K={K}: macro-F1={f1:.4f}\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 4.  choose best K\n",
    "best_K, best_f1 = max(scores, key=lambda t: t[1])\n",
    "print(f\"Best K = {best_K} (val macro-F1 = {best_f1:.4f})\")\n",
    "\n",
    "# plot curve\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(*zip(*scores), marker=\"o\")\n",
    "plt.title(\"Validation macro-F1 vs K\")\n",
    "plt.xlabel(\"Num. topics (K)\"); plt.ylabel(\"Macro F1\")\n",
    "plt.grid(True)\n",
    "plt.savefig(out_dir / \"val_f1_vs_K.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "print(\"Saved figure →\", out_dir / \"val_f1_vs_K.png\")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 5.  retrain BEST K on 25 % sample\n",
    "sss2 = StratifiedShuffleSplit(n_splits=1, train_size=0.25,\n",
    "                              random_state=random_state)\n",
    "idx2, _ = next(sss2.split(np.zeros(len(y_train)), y_train))\n",
    "bow_train_big = [bow_corpus_train[i] for i in idx2 if bow_corpus_train[i]]\n",
    "\n",
    "lda_best = train_lda(\n",
    "    bow_train_big, id2word,\n",
    "    k=best_K, passes=3, iterations=25,\n",
    "    workers=None\n",
    ")\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 6.  θ-vectors for ALL docs, save to disk\n",
    "X_train_vec = corpus_to_dense(lda_best, bow_corpus_train)\n",
    "X_test_vec  = corpus_to_dense(lda_best, bow_corpus_test)\n",
    "\n",
    "with gzip.open(\"theta_vectors_bestK.pkl.gz\", \"wb\") as f:\n",
    "    pickle.dump({\"train\": X_train_vec, \"test\": X_test_vec,\n",
    "                 \"K\": best_K, \"passes\":3, \"iterations\":25}, f,\n",
    "                protocol=pickle.HIGHEST_PROTOCOL)\n",
    "print(\"Saved θ-vectors → theta_vectors_bestK.pkl.gz\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced each class to 108929 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ifigeneiastathaki/Desktop/projects/LDA&regression/env/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
      "  warnings.warn(\n",
      "/Users/ifigeneiastathaki/Desktop/projects/LDA&regression/env/lib/python3.12/site-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5222484114230849\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1      0.249     0.705     0.369     41155\n",
      "           2      0.179     0.254     0.210     27232\n",
      "           3      0.215     0.254     0.233     45648\n",
      "           4      0.305     0.314     0.309    104056\n",
      "           5      0.821     0.611     0.700    381817\n",
      "\n",
      "    accuracy                          0.522    599908\n",
      "   macro avg      0.354     0.427     0.364    599908\n",
      "weighted avg      0.617     0.522     0.552    599908\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from gensim import matutils\n",
    "\n",
    "\n",
    "vocab_size = len(id2word)\n",
    "\n",
    "X_train_bow = matutils.corpus2csc(\n",
    "    bow_corpus_train,\n",
    "    num_terms=vocab_size,\n",
    "    dtype=float,\n",
    ").T                               \n",
    "\n",
    "X_test_bow = matutils.corpus2csc(\n",
    "    bow_corpus_test,\n",
    "    num_terms=vocab_size,\n",
    "    dtype=float,\n",
    ").T                                \n",
    "\n",
    "\n",
    "y_train = np.asarray(y_train)\n",
    "y_test  = np.asarray(y_test)\n",
    "\n",
    "\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "indices_by_class = defaultdict(list)\n",
    "for idx, label in enumerate(y_train):\n",
    "    indices_by_class[label].append(idx)\n",
    "\n",
    "min_size = min(len(lst) for lst in indices_by_class.values())\n",
    "\n",
    "balanced_idx = np.concatenate([\n",
    "    rng.choice(lst, size=min_size, replace=False)\n",
    "    for lst in indices_by_class.values()\n",
    "])\n",
    "\n",
    "X_balanced = X_train_bow[balanced_idx]  \n",
    "y_balanced = y_train[balanced_idx]\n",
    "\n",
    "print(\"Balanced each class to\", min_size, \"samples\")\n",
    "\n",
    "# Logistic regression training\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l2\",\n",
    "    C=1.0,\n",
    "    solver=\"saga\",           \n",
    "    max_iter=1000,\n",
    "    multi_class=\"multinomial\",\n",
    "    n_jobs=-1,          \n",
    "    random_state=42,\n",
    ")\n",
    "logreg.fit(X_balanced, y_balanced)\n",
    "\n",
    "# Evaluate on the full test set\n",
    "y_pred = logreg.predict(X_test_bow)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
